{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ed2e57-95f1-43c9-843f-4d19ae139fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = 6\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .config(\"spark.yarn.queue\", \"master_teams\")\\\n",
    "        .config(\"spark.executor.instances\", \"10\")\\\n",
    "        .config(\"spark.executor.cores\", \"10\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04995c87-b6af-44e1-ab8a-603f36292193",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read the hive tables as dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "643f643e-ab55-429e-a569-532992cc2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = spark.read.format(\"avro\").table('team6_projectdb.items')\n",
    "items.createOrReplaceTempView('items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0776a575-6d4d-4607-bda0-7b96d2835ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- itemid: integer (nullable = true)\n",
      " |-- shopid: integer (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- item_description: string (nullable = true)\n",
      " |-- item_variation: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- stock: integer (nullable = true)\n",
      " |-- cb_option: boolean (nullable = true)\n",
      " |-- is_preferred: boolean (nullable = true)\n",
      " |-- sold_count: integer (nullable = true)\n",
      " |-- item_creation_date: timestamp (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f83fa602-b696-4a55-a6de-475e172d7685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+--------------------+--------------+-----+-----+---------+------------+----------+-------------------+----------+\n",
      "|  itemid|  shopid|           item_name|    item_description|item_variation|price|stock|cb_option|is_preferred|sold_count| item_creation_date|  category|\n",
      "+--------+--------+--------------------+--------------------+--------------+-----+-----+---------+------------+----------+-------------------+----------+\n",
      "|88025115|11509993|Geometric Pattern...|Clothes Type:Padd...|            {}| 27.0|    0|     true|       false|         0|2016-11-09 16:01:00|Men's Wear|\n",
      "|88025112|11509993|Geometric Pattern...|Clothes Type:Padd...|            {}| 28.0|    1|     true|       false|         0|2016-11-09 16:01:00|Men's Wear|\n",
      "+--------+--------+--------------------+--------------------+--------------+-----+-----+---------+------------+----------+-------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ba593-5b31-4564-bcba-8332c223eb4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build and fit a feature extraction pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f57028-499a-4592-80a3-f8179862290e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vectorize Item Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bdf4c3d5-675b-433f-a026-6235dd3c0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_col = \"item_description\"\n",
    "tokens_col = \"desc_tokens\"\n",
    "output_col = \"desc_enc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90e4266f-b1ea-4d24-8983-369920a906e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+--------------------+-----+-----+---------+------------+----------+-------------------+-----------+--------------------+\n",
      "|   itemid|  shopid|           item_name|      item_variation|price|stock|cb_option|is_preferred|sold_count| item_creation_date|   category|         desc_tokens|\n",
      "+---------+--------+--------------------+--------------------+-----+-----+---------+------------+----------+-------------------+-----------+--------------------+\n",
      "|487531175|16174997|Creative Unisex F...|{40-41: 10.6, 36-...| 10.6| 5000|     true|       false|         0|2017-09-14 17:33:00|Men's Shoes|[specification:10...|\n",
      "|487531172|16174997|Creative Unisex F...|{28-29: 7.9, 30-3...|  7.9| 1000|     true|       false|         0|2017-09-14 17:33:00|Men's Shoes|[specification:10...|\n",
      "+---------+--------+--------------------+--------------------+-----+-----+---------+------------+----------+-------------------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(\n",
    "        inputCol=input_col,\n",
    "        outputCol=tokens_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec431d27-0e83-4b24-a65b-d1cb20e2feff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+---------------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+\n",
      "|   itemid|  shopid|           item_name| item_variation|price|stock|cb_option|is_preferred|sold_count| item_creation_date|       category|            desc_enc|\n",
      "+---------+--------+--------------------+---------------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+\n",
      "|581423084|16174997|Cross Stitch Tool...| {Default: 8.0}|  8.0| 1000|     true|       false|         3|2017-10-13 13:29:00|Design & Crafts|[-0.2476372364494...|\n",
      "|581423071|16174997|Cross Stitch Tool...|{Default: 11.4}| 11.4| 1000|     true|       false|         0|2017-10-13 13:29:00|Design & Crafts|[-0.1775885929167...|\n",
      "+---------+--------+--------------------+---------------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Word2Vec(\n",
    "        vectorSize=5,\n",
    "        seed=42,\n",
    "        minCount=10,\n",
    "        inputCol=tokens_col,\n",
    "        outputCol=output_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(tokens_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4da4fd-e8e4-4693-885a-8d2de15315e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vectorize Item Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90a34271-4521-40fe-85eb-b56d614c3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_col = \"item_variation\"\n",
    "tokens_col = \"var_tokens\"\n",
    "output_col = \"var_enc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "42fc1284-a1fd-4874-8c56-ca8d50acdf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+--------------------+--------------------+------------------+\n",
      "|   itemid|  shopid|           item_name|price|stock|cb_option|is_preferred|sold_count| item_creation_date|            category|            desc_enc|        var_tokens|\n",
      "+---------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+--------------------+--------------------+------------------+\n",
      "|534623211|16174997|2.4G Air Mouse Wi...|  7.8| 1000|     true|       false|         1|2017-09-30 07:22:00|Computers & Perip...|[-0.1966305688373...|   [{black:, 7.8}]|\n",
      "|455041448|16174997|2.4G Air Mouse Wi...| 12.0| 1000|     true|       false|         0|2017-09-01 08:55:00|Computers & Perip...|[-0.2856807188963...|[{default:, 12.0}]|\n",
      "+---------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+--------------------+--------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(\n",
    "        inputCol=input_col,\n",
    "        outputCol=tokens_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3fe4856-9c25-44be-8a66-0c6baa6d82f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+--------+--------------------+--------------------+\n",
      "|   itemid|  shopid|           item_name|price|stock|cb_option|is_preferred|sold_count| item_creation_date|category|            desc_enc|             var_enc|\n",
      "+---------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+--------+--------------------+--------------------+\n",
      "|744783814|16174997|SKMEI Mens Waterp...|14.02| 2000|     true|       false|         0|2017-12-09 15:43:00| Watches|[-0.2929523125930...|[-0.2924000592902...|\n",
      "|744783812|16174997|SKMEI Mens Waterp...|13.21| 2000|     true|       false|         0|2017-12-09 15:43:00| Watches|[-0.2977395940328...|[-0.0685735863633...|\n",
      "+---------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+--------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Word2Vec(\n",
    "        vectorSize=5,\n",
    "        seed=42,\n",
    "        minCount=1,\n",
    "        inputCol=tokens_col,\n",
    "        outputCol=output_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(tokens_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5554b-1fa4-4e9a-9b71-f1ac84bdf336",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vectorize Item Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83c80a1f-55ca-4e3f-8614-d58531decaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_col = \"item_name\"\n",
    "tokens_col = \"name_tokens\"\n",
    "output_col = \"name_enc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2672f7f-abde-4e5d-a60f-47077fbef9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+-------------+--------------------+--------------------+--------------------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count| item_creation_date|     category|            desc_enc|             var_enc|         name_tokens|\n",
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+-------------+--------------------+--------------------+--------------------+\n",
      "|455044645|16174997| 45.7| 1000|     true|       false|         0|2017-09-01 08:57:00|Home & Living|[-0.1504048824241...|[0.53768449742347...|[escam, q8, hd, 9...|\n",
      "|455044643|16174997| 45.7| 3000|     true|       false|         0|2017-09-01 08:57:00|Home & Living|[-0.1507162324350...|[0.53768449742347...|[escam, q8, hd, 9...|\n",
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(\n",
    "        inputCol=input_col,\n",
    "        outputCol=tokens_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00ce206c-a5d2-4a39-8347-ac2eab0a2f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count| item_creation_date|       category|            desc_enc|             var_enc|            name_enc|\n",
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "|229344431|20242999| 3.16|  100|     true|       false|         0|2017-04-21 23:00:00|Health & Beauty|[0.10731946888633...|[-0.0046485066413...|[-0.3792011671596...|\n",
      "|228869648|20242999|  3.2|  100|     true|       false|         0|2017-04-21 14:54:00|Health & Beauty|[0.06137828536045...|[-0.0046485066413...|[-0.3792011671596...|\n",
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Word2Vec(\n",
    "        vectorSize=5,\n",
    "        seed=42,\n",
    "        minCount=1,\n",
    "        inputCol=tokens_col,\n",
    "        outputCol=output_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(tokens_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c70857-1d2a-41d7-954f-54524f0a6b8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Encode timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4aee7cc9-4b2f-454d-958b-11344c1bbc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+--------+--------------------+--------------------+--------------------+----+-----+---+----+------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count|category|            desc_enc|             var_enc|            name_enc|year|month|day|hour|minute|\n",
      "+---------+--------+-----+-----+---------+------------+----------+--------+--------------------+--------------------+--------------------+----+-----+---+----+------+\n",
      "|744783814|16174997|14.02| 2000|     true|       false|         0| Watches|[-0.2929523125930...|[-0.2924000592902...|[-0.1417144838720...|2017|   12|  9|  15|    43|\n",
      "|744783812|16174997|13.21| 2000|     true|       false|         0| Watches|[-0.2977395940328...|[-0.0685735863633...|[-0.1417144838720...|2017|   12|  9|  15|    43|\n",
      "+---------+--------+-----+-----+---------+------------+----------+--------+--------------------+--------------------+--------------------+----+-----+---+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute\n",
    "\n",
    "# there is no sense in \"seconds\" column because it is always zero\n",
    "items = items.withColumn(\"year\", year(\"item_creation_date\")) \\\n",
    "        .withColumn(\"month\", month(\"item_creation_date\")) \\\n",
    "        .withColumn(\"day\", dayofmonth(\"item_creation_date\")) \\\n",
    "        .withColumn(\"hour\", hour(\"item_creation_date\")) \\\n",
    "        .withColumn(\"minute\", minute(\"item_creation_date\")) \\\n",
    "        .drop(\"item_creation_date\")\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5656b300-ee3c-41d5-a94f-a90812156f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "import math\n",
    "\n",
    "\n",
    "class TimeEncoderTransformer(\n",
    "    Transformer,\n",
    "    HasInputCol,\n",
    "    HasOutputCol,\n",
    "    DefaultParamsReadable,\n",
    "    DefaultParamsWritable\n",
    "):\n",
    "    input_col = Param(\n",
    "        Params._dummy(),\n",
    "        \"input_col\",\n",
    "        \"input column name.\",\n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "    output_col_sin = Param(\n",
    "        Params._dummy(),\n",
    "        \"output_col_sin\",\n",
    "        \"output column name for sin wave.\",\n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "    output_col_cos = Param(\n",
    "        Params._dummy(),\n",
    "        \"output_col_cos\",\n",
    "        \"output column name for cos wave.\",\n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "    timestamp_part = Param(\n",
    "        Params._dummy(),\n",
    "        \"timestamp_part\",\n",
    "        \"part of the timestamp like month, day, hour, minute\",\n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_col: str = \"input\",\n",
    "        output_col_sin: str = \"sin\",\n",
    "        output_col_cos: str = \"cos\",\n",
    "        timestamp_part: str = \"month\"\n",
    "    ):\n",
    "        super(TimeEncoderTransformer, self).__init__()\n",
    "        self._setDefault(\n",
    "            input_col=None,\n",
    "            output_col_sin=None,\n",
    "            output_col_cos=None,\n",
    "            timestamp_part=None\n",
    "        )\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def set_params(\n",
    "        self,\n",
    "        input_col: str = \"input\",\n",
    "        output_col_sin: str = \"sin\",\n",
    "        output_col_cos: str = \"cos\",\n",
    "        timestamp_part: str = \"month\"\n",
    "    ):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    def get_input_col(self):\n",
    "        return self.getOrDefault(self.input_col)\n",
    "\n",
    "    def get_output_col_sin(self):\n",
    "        return self.getOrDefault(self.output_col_sin)\n",
    "\n",
    "    def get_output_col_cos(self):\n",
    "        return self.getOrDefault(self.output_col_cos)\n",
    "\n",
    "    def get_timestamp_part(self):\n",
    "        return self.getOrDefault(self.timestamp_part)\n",
    "\n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_col = self.get_input_col()\n",
    "        output_col_sin = self.get_output_col_sin()\n",
    "        output_col_cos = self.get_output_col_cos()\n",
    "        timestamp_part = self.get_timestamp_part()\n",
    "\n",
    "        if timestamp_part == 'month':\n",
    "            denominator = 12\n",
    "        elif timestamp_part == 'day':\n",
    "            denominator = 31\n",
    "        elif timestamp_part in ['hour', 'minute']:\n",
    "            denominator = 60\n",
    "        else:\n",
    "            raise Exception()\n",
    "\n",
    "        sin_udf = F.udf(\n",
    "            lambda x: math.sin(2 * math.pi * x / denominator),\n",
    "            DoubleType()\n",
    "        )\n",
    "        cos_udf = F.udf(\n",
    "            lambda x: math.cos(2 * math.pi * x / denominator),\n",
    "            DoubleType()\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(output_col_sin, sin_udf(F.col(input_col)))\n",
    "        df = df.withColumn(output_col_cos, cos_udf(F.col(input_col)))\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab58de8b-bcaa-4544-be4d-646ef5f3de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+-----+---------+------------+----------+----------------+--------------------+--------------------+--------------------+----+---+----+------+--------------------+------------------+\n",
      "|   itemid| shopid|price|stock|cb_option|is_preferred|sold_count|        category|            desc_enc|             var_enc|            name_enc|year|day|hour|minute|           month_sin|         month_cos|\n",
      "+---------+-------+-----+-----+---------+------------+----------+----------------+--------------------+--------------------+--------------------+----+---+----+------+--------------------+------------------+\n",
      "|821115857|3344977| 1.25|   20|    false|       false|         0|Food & Beverages|[-0.0847395360469...|[-0.0046485066413...|[-0.2669184163212...|2018|  9|  18|     8| 0.49999999999999994|0.8660254037844387|\n",
      "|780592157|3344977| 1.25|   90|    false|       false|         0|Food & Beverages|[-0.2529235662598...|[0.12054621366163...|[-0.2669184163212...|2017| 23|   7|    59|-2.44929359829470...|               1.0|\n",
      "+---------+-------+-----+-----+---------+------------+----------+----------------+--------------------+--------------------+--------------------+----+---+----+------+--------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_col = \"month\"\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    TimeEncoderTransformer(\n",
    "        input_col=input_col,\n",
    "        output_col_sin=f\"{input_col}_sin\",\n",
    "        output_col_cos=f\"{input_col}_cos\",\n",
    "        timestamp_part=input_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "de79a222-07c1-4a13-94d5-79f200098751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+--------+--------------------+--------------------+--------------------+----+----+------+--------------------+---------+------------------+-------------------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count|category|            desc_enc|             var_enc|            name_enc|year|hour|minute|           month_sin|month_cos|           day_sin|            day_cos|\n",
      "+---------+--------+-----+-----+---------+------------+----------+--------+--------------------+--------------------+--------------------+----+----+------+--------------------+---------+------------------+-------------------+\n",
      "|744783814|16174997|14.02| 2000|     true|       false|         0| Watches|[-0.2929523125930...|[-0.2924000592902...|[-0.1417144838720...|2017|  15|    43|-2.44929359829470...|      1.0|0.9680771188662043|-0.2506525322587204|\n",
      "|744783812|16174997|13.21| 2000|     true|       false|         0| Watches|[-0.2977395940328...|[-0.0685735863633...|[-0.1417144838720...|2017|  15|    43|-2.44929359829470...|      1.0|0.9680771188662043|-0.2506525322587204|\n",
      "+---------+--------+-----+-----+---------+------------+----------+--------+--------------------+--------------------+--------------------+----+----+------+--------------------+---------+------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_col = \"day\"\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    TimeEncoderTransformer(\n",
    "        input_col=input_col,\n",
    "        output_col_sin=f\"{input_col}_sin\",\n",
    "        output_col_cos=f\"{input_col}_cos\",\n",
    "        timestamp_part=input_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e192bd2a-ebef-4e6f-90a3-b224d131a45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+-----+---------+------------+----------+----------+--------------------+--------------------+--------------------+----+------+-------------------+------------------+------------------+-------------------+------------------+--------------------+\n",
      "|  itemid|  shopid|price|stock|cb_option|is_preferred|sold_count|  category|            desc_enc|             var_enc|            name_enc|year|minute|          month_sin|         month_cos|           day_sin|            day_cos|          hour_sin|            hour_cos|\n",
      "+--------+--------+-----+-----+---------+------------+----------+----------+--------------------+--------------------+--------------------+----+------+-------------------+------------------+------------------+-------------------+------------------+--------------------+\n",
      "|88025115|11509993| 27.0|    0|     true|       false|         0|Men's Wear|[0.03715182226151...|[-0.0046485066413...|[-0.4407809637486...|2016|     1|-0.5000000000000004|0.8660254037844384|0.9680771188662043|-0.2506525322587204|0.9945218953682734|-0.10452846326765333|\n",
      "|88025112|11509993| 28.0|    1|     true|       false|         0|Men's Wear|[0.03715182226151...|[-0.0046485066413...|[-0.4407809637486...|2016|     1|-0.5000000000000004|0.8660254037844384|0.9680771188662043|-0.2506525322587204|0.9945218953682734|-0.10452846326765333|\n",
      "+--------+--------+-----+-----+---------+------------+----------+----------+--------------------+--------------------+--------------------+----+------+-------------------+------------------+------------------+-------------------+------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_col = \"hour\"\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    TimeEncoderTransformer(\n",
    "        input_col=input_col,\n",
    "        output_col_sin=f\"{input_col}_sin\",\n",
    "        output_col_cos=f\"{input_col}_cos\",\n",
    "        timestamp_part=input_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "62ad55db-7751-45c1-a1da-53e6694605db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+-------------+--------------------+--------------------+--------------------+----+------+---------+--------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count|     category|            desc_enc|             var_enc|            name_enc|year|minute|month_sin|           month_cos|            day_sin|           day_cos|          hour_sin|          hour_cos|         minute_sin|        minute_cos|\n",
      "+---------+--------+-----+-----+---------+------------+----------+-------------+--------------------+--------------------+--------------------+----+------+---------+--------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+\n",
      "|455044645|16174997| 45.7| 1000|     true|       false|         0|Home & Living|[-0.1504048824241...|[0.53768449742347...|[0.10033358387382...|2017|    57|     -1.0|-1.83697019872102...|0.20129852008866006|0.9795299412524945|0.7431448254773941|0.6691306063588582|-0.3090169943749476|0.9510565162951535|\n",
      "|455044643|16174997| 45.7| 3000|     true|       false|         0|Home & Living|[-0.1507162324350...|[0.53768449742347...|[0.10033358387382...|2017|    57|     -1.0|-1.83697019872102...|0.20129852008866006|0.9795299412524945|0.7431448254773941|0.6691306063588582|-0.3090169943749476|0.9510565162951535|\n",
      "+---------+--------+-----+-----+---------+------------+----------+-------------+--------------------+--------------------+--------------------+----+------+---------+--------------------+-------------------+------------------+------------------+------------------+-------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_col = \"minute\"\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    TimeEncoderTransformer(\n",
    "        input_col=input_col,\n",
    "        output_col_sin=f\"{input_col}_sin\",\n",
    "        output_col_cos=f\"{input_col}_cos\",\n",
    "        timestamp_part=input_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c6704-dd75-4784-b6dd-b0532d339d85",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Encode Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f12e342a-3beb-410f-8e23-cdba9d9f2b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+--------------------+--------------------+--------------------+----+------+---------+--------------------+------------------+-------------------+-------------------+--------------------+-------------------+-------------------+----------------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count|            desc_enc|             var_enc|            name_enc|year|minute|month_sin|           month_cos|           day_sin|            day_cos|           hour_sin|            hour_cos|         minute_sin|         minute_cos|category_indexed|\n",
      "+---------+--------+-----+-----+---------+------------+----------+--------------------+--------------------+--------------------+----+------+---------+--------------------+------------------+-------------------+-------------------+--------------------+-------------------+-------------------+----------------+\n",
      "|185120551|16503999|  8.0|    5|    false|       false|         0|[0.06638372838497...|[0.11030323306719...|[0.22160092741250...|2017|    26|      1.0|6.123233995736766...|-0.651372482722222|-0.7587581226927911|  0.913545457642601|-0.40673664307580004|0.40673664307580004| -0.913545457642601|             1.0|\n",
      "|169964291|16503999|  8.0|    4|    false|       false|         0|[-0.0214091129601...|[0.10029645636677...|[0.22160092741250...|2017|    49|      1.0|6.123233995736766...|0.9884683243281114| 0.1514277775045767|0.20791169081775931|  0.9781476007338057|-0.9135454576426011|0.40673664307579976|             1.0|\n",
      "+---------+--------+-----+-----+---------+------------+----------+--------------------+--------------------+--------------------+----+------+---------+--------------------+------------------+-------------------+-------------------+--------------------+-------------------+-------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_col = \"category\"\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    StringIndexer(\n",
    "        inputCol=input_col,\n",
    "        outputCol=f\"{input_col}_indexed\")\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2e74e8f-0240-4044-8bf4-63fcdd7ad161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- itemid: integer (nullable = true)\n",
      " |-- shopid: integer (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- stock: integer (nullable = true)\n",
      " |-- cb_option: boolean (nullable = true)\n",
      " |-- is_preferred: boolean (nullable = true)\n",
      " |-- sold_count: integer (nullable = true)\n",
      " |-- desc_enc: vector (nullable = true)\n",
      " |-- var_enc: vector (nullable = true)\n",
      " |-- name_enc: vector (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- month_sin: double (nullable = true)\n",
      " |-- month_cos: double (nullable = true)\n",
      " |-- day_sin: double (nullable = true)\n",
      " |-- day_cos: double (nullable = true)\n",
      " |-- hour_sin: double (nullable = true)\n",
      " |-- hour_cos: double (nullable = true)\n",
      " |-- minute_sin: double (nullable = true)\n",
      " |-- minute_cos: double (nullable = true)\n",
      " |-- category_indexed: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06deb11a-ab6f-420b-834a-919e8a2b5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS items_feat_extracted (\n",
    "    itemid INTEGER,\n",
    "    shopid INTEGER,\n",
    "    price FLOAT,\n",
    "    stock INTEGER,\n",
    "    cb_option BOOLEAN,\n",
    "    is_oreferred BOOLEAN,\n",
    "    sold_count INTEGER,\n",
    "    desc_enc ARRAY<DOUBLE>,\n",
    "    var_enc ARRAY<DOUBLE>,\n",
    "    name_enc ARRAY<DOUBLE>,\n",
    "    year INTEGER,\n",
    "    month_sin DOUBLE,\n",
    "    month_cos DOUBLE,\n",
    "    day_sin DOUBLE,\n",
    "    day_cos DOUBLE,\n",
    "    hour_sin DOUBLE,\n",
    "    hour_cos DOUBLE,\n",
    "    minute_sin DOUBLE,\n",
    "    minute_cos DOUBLE,\n",
    "    category_indexed DOUBLE\n",
    ")\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'project/hive/warehouse/items_feat_extracted'\n",
    "\"\"\"\n",
    "\n",
    "# spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "49cffff2-4bc8-4be0-99dc-d4cd0d8f2811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0: jdbc:hive2://hadoop-03.uni.innopolis.ru:10> '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def run(command):\n",
    "    return os.popen(command).read()\n",
    "\n",
    "with open('../scripts/secrets/.hive.pass') as f:\n",
    "    password = f.read()\n",
    "\n",
    "run(f\"beeline -u jdbc:hive2://hadoop-03.uni.innopolis.ru:10001 -n team6 -p {password} -e {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9f03226d-631a-4783-9208-b951c8848a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "items.limit(1)\\\n",
    "        .write\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .saveAsTable(\"team6_projectdb.items_feat_extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eadcc6-c656-440b-9411-386b6e3fe595",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Assemble Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43c335-b332-4a9f-b94e-212ca42ad705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "inputCols = [\n",
    "    'itemid',\n",
    "    'shopid',\n",
    "    'price',\n",
    "    'stock',\n",
    "    'cb_option',\n",
    "    'is_preferred',\n",
    "    'sold_count',\n",
    "    'year',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'hour_sin',\n",
    "    'hour_cos',\n",
    "    'minute_sin',\n",
    "    'minute_cos',\n",
    "    'name_enc',\n",
    "    'desc_enc',\n",
    "    'var_enc'\n",
    "]\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    VectorAssembler(\n",
    "        inputCols=inputCols,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items).transform(items)\n",
    "\n",
    "for col in inputCols:\n",
    "    items = items.drop(col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7791c-ef0e-4386-8346-afacbe9f59bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split the input dataset into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12c5d13b-f541-48be-b107-0aba4445c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = items.randomSplit([0.8, 0.2], seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "679f431d-eb93-464d-9756-1af24ad9a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.select(\"features\", \"category_indexed\")\\\n",
    "    .repartition(4)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "!hdfs dfs -get project/data/train/*.json ../data/train\n",
    "\n",
    "test_data.select(\"features\", \"category_indexed\")\\\n",
    "    .repartition(4)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/test\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "!hdfs dfs -get project/data/test/*.json ../data/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10362844-9fb9-41ca-bc2d-3b5fc8c69897",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Select two types of ML models based on the ML task specified in project.info sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab7668-d720-4fd6-a6b4-ee4e31e96cee",
   "metadata": {},
   "source": [
    "1. Logistic Regression\n",
    "2. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9de1a2-7e15-4a08-962b-57e84d062591",
   "metadata": {
    "tags": []
   },
   "source": [
    "## First model type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23c396-50ee-4662-a897-63aaa616250a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9ac0681-8cea-42ae-bcea-7ee86f029368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"category_indexed\", featuresCol=\"features\")\n",
    "model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5654cdf-a871-40c2-b527-1044ba98d0dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75c55976-683c-4ec8-98ff-cafe0054d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9905960b-b4a6-43dd-86da-1afb7701e8f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "320e2fc5-bd71-49e3-9ce8-7f4a0317c947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1-score: 0.6388818106996959\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate the model using F1-score\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(f\"Test set F1-score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d23263b-cceb-4804-adcb-4bd4fa227eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|category_indexed|\n",
      "+----------------+\n",
      "|            16.0|\n",
      "|            15.0|\n",
      "|             3.0|\n",
      "|            14.0|\n",
      "|            18.0|\n",
      "|             0.0|\n",
      "|             8.0|\n",
      "|             2.0|\n",
      "|            13.0|\n",
      "|            19.0|\n",
      "|             9.0|\n",
      "|            12.0|\n",
      "|            10.0|\n",
      "|             1.0|\n",
      "|             5.0|\n",
      "|            11.0|\n",
      "|             7.0|\n",
      "|             6.0|\n",
      "|            17.0|\n",
      "|             4.0|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = items.select('category_indexed').distinct()\n",
    "categories.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "250cada6-21ab-418f-8de1-f89006bced0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set precision for category 0: 0.683888447704022\n",
      "Test set precision for category 1: 0.8509016826378933\n",
      "Test set precision for category 2: 0.6119703062171358\n",
      "Test set precision for category 3: 0.7564494971578487\n",
      "Test set precision for category 4: 0.3930151338766007\n",
      "Test set precision for category 5: 0.38817733990147785\n",
      "Test set precision for category 6: 0.6101610493109746\n",
      "Test set precision for category 7: 0.6299212598425197\n",
      "Test set precision for category 8: 0.7067178502879079\n",
      "Test set precision for category 9: 0.5828260173021468\n",
      "Test set precision for category 10: 0.81212976022567\n",
      "Test set precision for category 11: 0.31530139103554866\n",
      "Test set precision for category 12: 0.1553398058252427\n",
      "Test set precision for category 13: 0.3215258855585831\n",
      "Test set precision for category 14: 0.5639810426540285\n",
      "Test set precision for category 15: 0.25862068965517243\n",
      "Test set precision for category 16: 0.0\n",
      "Test set precision for category 17: 0.6\n",
      "Test set precision for category 18: 0.14285714285714285\n",
      "Test set precision for category 19: 0.0\n",
      "Test set precision for category 20: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using per-category precision\n",
    "for metricLabel in range(21):\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"category_indexed\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"precisionByLabel\",\n",
    "        metricLabel=metricLabel\n",
    "    )\n",
    "    precisionByLabel = evaluator.evaluate(predictions)\n",
    "    print(f\"Test set precision for category {metricLabel}: {precisionByLabel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36517019-c378-4879-bc4f-1ade2cded746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set recall for category 0: 0.8454311085648811\n",
      "Test set recall for category 1: 0.9136352588511707\n",
      "Test set recall for category 2: 0.72397490680971\n",
      "Test set recall for category 3: 0.638945991403476\n",
      "Test set recall for category 4: 0.28433814980900185\n",
      "Test set recall for category 5: 0.463402489626556\n",
      "Test set recall for category 6: 0.6575713769425371\n",
      "Test set recall for category 7: 0.629147571035747\n",
      "Test set recall for category 8: 0.7471287940935193\n",
      "Test set recall for category 9: 0.45863900834809007\n",
      "Test set recall for category 10: 0.836563494359271\n",
      "Test set recall for category 11: 0.11639549436795996\n",
      "Test set recall for category 12: 0.047653429602888084\n",
      "Test set recall for category 13: 0.09515570934256055\n",
      "Test set recall for category 14: 0.1631644004944376\n",
      "Test set recall for category 15: 0.029469548133595286\n",
      "Test set recall for category 16: 0.0\n",
      "Test set recall for category 17: 0.009592326139088728\n",
      "Test set recall for category 18: 0.0078125\n",
      "Test set recall for category 19: 0.0\n",
      "Test set recall for category 20: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using per-category recall\n",
    "for metricLabel in range(21):\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"category_indexed\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"recallByLabel\",\n",
    "        metricLabel=metricLabel\n",
    "    )\n",
    "    recallByLabel = evaluator.evaluate(predictions)\n",
    "    print(f\"Test set recall for category {metricLabel}: {recallByLabel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f2c67-53f3-4ec0-864b-db93c1e2b76d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specify at least 2 hyperparameters for it and the settings of grid search and cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f17c6f-7faa-4fbc-aa0c-52e566505e61",
   "metadata": {},
   "source": [
    "1. RegParam\n",
    "2. ElasticNetParam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7c361-11d5-4813-9ead-3b3f639f1ddf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optimize its hyperparameters using cross validation and grid search on the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f83c9260-b4e9-4fc9-b704-5d388117bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c1d33d6-5b53-45cf-b043-d4cc49b435dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cc21e7c-e08b-43dd-8a79-072bfe2b883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Create the CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e70b5fcf-91e5-4269-be90-0d6aa6aa7a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using cross-validation on the training data\n",
    "cvModel = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e64f86aa-eff9-4f50-92f4-6e8038d69c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ../output/model1_hyperparameters: File exists\n",
      "get: `../output/model1_hyperparameters/_SUCCESS': File exists\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"parameter\", StringType(), False),\n",
    "    StructField(\"value\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"RegParam\", cvModel.bestModel.getRegParam()),\n",
    "    (\"ElasticNetParam\", cvModel.bestModel.getElasticNetParam())\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .save(\"project/output/model1_hyperparameters\")\n",
    "\n",
    "!rm -rf ../output/model1_hyperparameters \n",
    "!mkdir ../output/model1_hyperparameters\n",
    "!hdfs dfs -get project/output/model1_hyperparameters/* ../output/model1_hyperparameters/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08330fc-2ce5-43f0-a33b-a52f5db35ff2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select the best model (model1) from grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2c1c2bf-cf84-457d-b836-7136585fdec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55fb0ad-2da3-426e-94bc-e30e4ff3f101",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save the model1 to HDFS in location like project/models/model1 and later put it in models/model1 folder in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22d2d42a-822d-4a35-8cde-49d2a6edda3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.write().overwrite().save(\"project/models/model1\")\n",
    "\n",
    "!hdfs dfs -get project/models/model1 ../models/model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf9e9b-c132-4f09-9ec8-36e3272aa36a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict for the test data using the model1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "797053a9-23c5-4b42-8920-e22747786d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = bestModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db097221-af2f-481e-866f-7b55ca7829fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save the prediction results in HDFS in a CSV file like project/output/model1_predictions and later save it in output/model1_predictions.csv folder in the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6530cc8a-b901-4542-a25e-3c83b354f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"category_indexed\", \"prediction\")\\\n",
    "    .repartition(4)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .save(\"project/output/model1_predictions\")\n",
    "\n",
    "!rm -rf ../output/model1_predictions\n",
    "!mkdir ../output/model1_predictions\n",
    "!hdfs dfs -get project/output/model1_predictions/* ../output/model1_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a8e45-87bc-4bae-8b21-0fd6cb821186",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the best model (model1) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "460a5e41-3c58-4589-ba99-0a0ef83ddcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1-score: 0.6361978263932841\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = bestModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model using F1-score\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1_score_1 = evaluator.evaluate(predictions)\n",
    "print(f\"Test set F1-score: {f1_score_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38811325-2aa4-4c2d-9b4e-5f42aaab65bf",
   "metadata": {},
   "source": [
    "## Second model type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3df8a1-006b-4ad3-a486-1c509b6076aa",
   "metadata": {},
   "source": [
    "### Build and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de23c149-d192-4581-94ad-c2bbed3db2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"category_indexed\", featuresCol=\"features\")\n",
    "model = dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b95ff-d240-45c9-b99d-f66dde6f1a50",
   "metadata": {},
   "source": [
    "### Predict for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ae9ce4e-e4be-4ead-b5d9-759b604a0e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f4e2e-c175-48d8-8c6b-b75335fc7558",
   "metadata": {},
   "source": [
    "### Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7d9fd9c-f3bd-4c13-842a-fc590febcde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1-score: 0.518044828295119\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate the model using F1-score\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(f\"Test set F1-score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d913f70-08ce-4971-bde0-e168bb0c9ef7",
   "metadata": {},
   "source": [
    "### Specify at least 2 hyperparameters for it and the settings of grid search and cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d494b3-0909-4376-8956-bd391b14e5fe",
   "metadata": {},
   "source": [
    "1. MaxDepth\n",
    "2. MaxInfoGain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d1ae4-3698-4a9e-95f9-4ee467720d41",
   "metadata": {},
   "source": [
    "### Optimize its hyperparameters using cross validation and grid search on the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d92a7476-aa02-49b3-947e-3ec089122410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(dt.minInfoGain, [0.0, 0.01, 0.1]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff1a8d51-669a-4836-84d5-50d444b2c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "17bb9d36-3cb9-42c1-bb00-fb08fecf7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Create the CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=dt,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "96ca761a-bf01-44d2-957b-cc085e00d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using cross-validation on the training data\n",
    "cvModel = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "29253008-e115-455b-9402-2db601317522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [\n",
    "    (\"MaxDepth\", cvModel.bestModel.getMaxDepth()),\n",
    "    (\"MinInfoGain\", cvModel.bestModel.getMinInfoGain())\n",
    "]\n",
    "\n",
    "# Convert integer values to double\n",
    "data = [(k, float(v)) for k, v in data]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .save(\"project/output/model2_hyperparameters\")\n",
    "\n",
    "!rm -rf ../output/model2_hyperparameters \n",
    "!mkdir ../output/model2_hyperparameters\n",
    "!hdfs dfs -get project/output/model2_hyperparameters/* ../output/model2_hyperparameters/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5bdb12-3bb2-4943-9e43-fa62f968b6bc",
   "metadata": {},
   "source": [
    "### Select the best model (model2) from grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a15404c-4cf3-4728-a905-145b384730c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6a73f-8b4a-44e1-b7f0-4c567f62d695",
   "metadata": {},
   "source": [
    "### Save the model2 to HDFS in location like project/models/model2 and later put it in models/model2 folder in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1db79063-ffc0-492f-8711-162ab64fca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.write().overwrite().save(\"project/models/model2\")\n",
    "\n",
    "!hdfs dfs -get project/models/model2 ../models/model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8359d-8b4b-4cfb-919a-76a16b7c48cd",
   "metadata": {},
   "source": [
    "### Predict for the test data using the model2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9736139-4e55-4f35-a36c-ccf05091a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = bestModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745dbe0-04cd-4f59-95f6-74c132c70117",
   "metadata": {},
   "source": [
    "### Save the prediction results in HDFS in a CSV file like project/output/model2_predictions and later save it in output/model2_predictions.csv folder in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8eb428d7-dd67-4c85-a542-f9bdf55c4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"category_indexed\", \"prediction\")\\\n",
    "    .repartition(4)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .save(\"project/output/model2_predictions\")\n",
    "\n",
    "!rm -rf ../output/model2_predictions \n",
    "!mkdir ../output/model2_predictions\n",
    "!hdfs dfs -get project/output/model2_predictions/* ../output/model2_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f48cd-c943-4316-a5fc-944386d66f42",
   "metadata": {},
   "source": [
    "### Evaluate the best model (model2) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f49e0175-6f8b-4ee9-9f07-0968353e1891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1-score: 0.6746254145968918\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = bestModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model using F1-score\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1_score_2 = evaluator.evaluate(predictions)\n",
    "print(f\"Test set F1-score: {f1_score_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4791f-8713-47e6-96e5-15a6e7618ec1",
   "metadata": {},
   "source": [
    "## Compare the models (model1, model2) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe1501d1-82df-4a8f-a4aa-46632611f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"model\", StringType(), False),\n",
    "    StructField(\"f1_score\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"Logistic Regression\", f1_score_1),\n",
    "    (\"Decision Tree\", f1_score_2)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35fd7349-8810-4764-ad41-32374ed1339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .save(\"project/output/evaluation\")\n",
    "\n",
    "!rm -rf ../output/comparison \n",
    "!mkdir ../output/comparison\n",
    "!hdfs dfs -get project/output/evaluation/* ../output/comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa3d91-e67e-4c45-adee-e34a07717557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
