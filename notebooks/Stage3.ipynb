{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ed2e57-95f1-43c9-843f-4d19ae139fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = 6\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .config(\"spark.yarn.queue\", \"master_teams\")\\\n",
    "        .config(\"spark.executor.instances\", \"10\")\\\n",
    "        .config(\"spark.executor.cores\", \"10\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04995c87-b6af-44e1-ab8a-603f36292193",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read the hive tables as dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643f643e-ab55-429e-a569-532992cc2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = spark.read.format(\"avro\").table('team6_projectdb.items')\n",
    "items.createOrReplaceTempView('items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0776a575-6d4d-4607-bda0-7b96d2835ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- itemid: integer (nullable = true)\n",
      " |-- shopid: integer (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- item_description: string (nullable = true)\n",
      " |-- item_variation: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- stock: integer (nullable = true)\n",
      " |-- cb_option: boolean (nullable = true)\n",
      " |-- is_preferred: boolean (nullable = true)\n",
      " |-- sold_count: integer (nullable = true)\n",
      " |-- item_creation_date: timestamp (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83fa602-b696-4a55-a6de-475e172d7685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+--------------------+--------------+-----+-----+---------+------------+----------+-------------------+--------------+\n",
      "|   itemid|  shopid|           item_name|    item_description|item_variation|price|stock|cb_option|is_preferred|sold_count| item_creation_date|      category|\n",
      "+---------+--------+--------------------+--------------------+--------------+-----+-----+---------+------------+----------+-------------------+--------------+\n",
      "|682899825|16174997|Christmas Sexy Sl...|Christmas Sexy Sl...|{Default: 4.0}|  4.0| 1000|     true|       false|         0|2017-11-14 14:22:00|Miscellaneous |\n",
      "|682899784|16174997|Christmas Sexy Sl...|Christmas Sexy Sl...|{Default: 4.0}|  4.0| 1000|     true|       false|         0|2017-11-14 14:22:00|Miscellaneous |\n",
      "+---------+--------+--------------------+--------------------+--------------+-----+-----+---------+------------+----------+-------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ba593-5b31-4564-bcba-8332c223eb4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build and fit a feature extraction pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f57028-499a-4592-80a3-f8179862290e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vectorize Item Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf4c3d5-675b-433f-a026-6235dd3c0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_col = \"item_description\"\n",
    "tokens_col = \"desc_tokens\"\n",
    "output_col = \"desc_enc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e4266f-b1ea-4d24-8983-369920a906e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+--------------------+-----+-----+---------+------------+----------+-------------------+----------------+--------------------+\n",
      "|   itemid| shopid|           item_name|      item_variation|price|stock|cb_option|is_preferred|sold_count| item_creation_date|        category|         desc_tokens|\n",
      "+---------+-------+--------------------+--------------------+-----+-----+---------+------------+----------+-------------------+----------------+--------------------+\n",
      "|821115857|3344977|Minute Maid Pulph...|                  {}| 1.25|   20|    false|       false|         0|2018-01-09 18:08:00|Food & Beverages|[free, delivery, ...|\n",
      "|780592157|3344977|Minute Maid Pulph...|{Orange: 1.25, Ma...| 1.25|   90|    false|       false|         0|2017-12-23 07:59:00|Food & Beverages|[ingredients:wate...|\n",
      "+---------+-------+--------------------+--------------------+-----+-----+---------+------------+----------+-------------------+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(\n",
    "        inputCol=input_col,\n",
    "        outputCol=tokens_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec431d27-0e83-4b24-a65b-d1cb20e2feff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+---------------+-----+-----+---------+------------+----------+-------------------+--------------------+--------------------+\n",
      "|   itemid|  shopid|           item_name| item_variation|price|stock|cb_option|is_preferred|sold_count| item_creation_date|            category|            desc_enc|\n",
      "+---------+--------+--------------------+---------------+-----+-----+---------+------------+----------+-------------------+--------------------+--------------------+\n",
      "|534623211|16174997|2.4G Air Mouse Wi...|   {Black: 7.8}|  7.8| 1000|     true|       false|         1|2017-09-30 07:22:00|Computers & Perip...|[-0.1520559997172...|\n",
      "|455041448|16174997|2.4G Air Mouse Wi...|{DEFAULT: 12.0}| 12.0| 1000|     true|       false|         0|2017-09-01 08:55:00|Computers & Perip...|[-0.1403763069035...|\n",
      "+---------+--------+--------------------+---------------+-----+-----+---------+------------+----------+-------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Word2Vec(\n",
    "        vectorSize=5,\n",
    "        seed=42,\n",
    "        minCount=10,\n",
    "        inputCol=tokens_col,\n",
    "        outputCol=output_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(tokens_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4da4fd-e8e4-4693-885a-8d2de15315e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vectorize Item Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a34271-4521-40fe-85eb-b56d614c3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_col = \"item_variation\"\n",
    "tokens_col = \"var_tokens\"\n",
    "output_col = \"var_enc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42fc1284-a1fd-4874-8c56-ca8d50acdf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+-----------------+\n",
      "|   itemid|  shopid|           item_name|price|stock|cb_option|is_preferred|sold_count| item_creation_date|       category|            desc_enc|       var_tokens|\n",
      "+---------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+-----------------+\n",
      "|372745192|16174997|1/4 3/8 1/2 8Pcs ...|  2.7| 1000|     true|       false|         0|2017-07-21 09:58:00|Home Appliances|[-0.1936963422348...|[{default:, 2.7}]|\n",
      "|372743622|16174997|1/4 3/8 1/2 8Pcs ...|  3.9| 1000|     true|       false|         0|2017-07-21 09:57:00|Home Appliances|[-0.1218324403499...|[{default:, 3.9}]|\n",
      "+---------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(\n",
    "        inputCol=input_col,\n",
    "        outputCol=tokens_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3fe4856-9c25-44be-8a66-0c6baa6d82f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+-----------------+--------------------+--------------------+\n",
      "|    itemid|  shopid|           item_name|price|stock|cb_option|is_preferred|sold_count| item_creation_date|         category|            desc_enc|             var_enc|\n",
      "+----------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+-----------------+--------------------+--------------------+\n",
      "|1015202788|16174997|HW Fishing Tackle...| 2.24| 1000|     true|       false|         0|2018-03-26 07:58:00|Sports & Outdoors|[-0.3909013677956...|[0.13756516203284...|\n",
      "|1015202786|16174997|HW Fishing Tackle...| 2.58| 1000|     true|       false|         0|2018-03-26 07:58:00|Sports & Outdoors|[-0.4445349556409...|[0.01176970079541...|\n",
      "+----------+--------+--------------------+-----+-----+---------+------------+----------+-------------------+-----------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Word2Vec(\n",
    "        vectorSize=5,\n",
    "        seed=42,\n",
    "        minCount=1,\n",
    "        inputCol=tokens_col,\n",
    "        outputCol=output_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(tokens_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5554b-1fa4-4e9a-9b71-f1ac84bdf336",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vectorize Item Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c80a1f-55ca-4e3f-8614-d58531decaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_col = \"item_name\"\n",
    "tokens_col = \"name_tokens\"\n",
    "output_col = \"name_enc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2672f7f-abde-4e5d-a60f-47077fbef9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count| item_creation_date|       category|            desc_enc|             var_enc|         name_tokens|\n",
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "|839669074|16174997| 2.52| 2000|     true|       false|         0|2018-01-16 13:32:00|Pet Accessories|[-0.3230378506332...|[-0.0945925083942...|[handle, shedding...|\n",
      "|839669045|16174997|  2.5| 2000|     true|       false|         0|2018-01-16 13:32:00|Pet Accessories|[-0.3198394560725...|[-0.2202361963689...|[handle, shedding...|\n",
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+---------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Tokenizer(\n",
    "        inputCol=input_col,\n",
    "        outputCol=tokens_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00ce206c-a5d2-4a39-8347-ac2eab0a2f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+-------------+--------------------+--------------------+--------------------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count| item_creation_date|     category|            desc_enc|             var_enc|            name_enc|\n",
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+-------------+--------------------+--------------------+--------------------+\n",
      "|455044645|16174997| 45.7| 1000|     true|       false|         0|2017-09-01 08:57:00|Home & Living|[-0.0499512929065...|[0.00447766948491...|[0.58400923599089...|\n",
      "|455044643|16174997| 45.7| 3000|     true|       false|         0|2017-09-01 08:57:00|Home & Living|[-0.0482056512954...|[0.00447766948491...|[0.58400923599089...|\n",
      "+---------+--------+-----+-----+---------+------------+----------+-------------------+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    Word2Vec(\n",
    "        vectorSize=5,\n",
    "        seed=42,\n",
    "        minCount=1,\n",
    "        inputCol=tokens_col,\n",
    "        outputCol=output_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(tokens_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c70857-1d2a-41d7-954f-54524f0a6b8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Encode timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aee7cc9-4b2f-454d-958b-11344c1bbc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+---------------+--------------------+--------------------+--------------------+----+-----+---+----+------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count|       category|            desc_enc|             var_enc|            name_enc|year|month|day|hour|minute|\n",
      "+---------+--------+-----+-----+---------+------------+----------+---------------+--------------------+--------------------+--------------------+----+-----+---+----+------+\n",
      "|839669074|16174997| 2.52| 2000|     true|       false|         0|Pet Accessories|[-0.3230378506332...|[-0.0945925083942...|[0.33843006609151...|2018|    1| 16|  13|    32|\n",
      "|839669045|16174997|  2.5| 2000|     true|       false|         0|Pet Accessories|[-0.3198394560725...|[-0.2202361963689...|[0.33843006609151...|2018|    1| 16|  13|    32|\n",
      "+---------+--------+-----+-----+---------+------------+----------+---------------+--------------------+--------------------+--------------------+----+-----+---+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute\n",
    "\n",
    "# there is no sense in \"seconds\" column because it is always zero\n",
    "items = items.withColumn(\"year\", year(\"item_creation_date\")) \\\n",
    "        .withColumn(\"month\", month(\"item_creation_date\")) \\\n",
    "        .withColumn(\"day\", dayofmonth(\"item_creation_date\")) \\\n",
    "        .withColumn(\"hour\", hour(\"item_creation_date\")) \\\n",
    "        .withColumn(\"minute\", minute(\"item_creation_date\")) \\\n",
    "        .drop(\"item_creation_date\")\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5656b300-ee3c-41d5-a94f-a90812156f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "import math\n",
    "\n",
    "\n",
    "class TimeEncoderTransformer(\n",
    "    Transformer,\n",
    "    HasInputCol,\n",
    "    HasOutputCol,\n",
    "    DefaultParamsReadable,\n",
    "    DefaultParamsWritable\n",
    "):\n",
    "    input_col = Param(\n",
    "        Params._dummy(),\n",
    "        \"input_col\",\n",
    "        \"input column name.\",\n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "    output_col_sin = Param(\n",
    "        Params._dummy(),\n",
    "        \"output_col_sin\",\n",
    "        \"output column name for sin wave.\",\n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "    output_col_cos = Param(\n",
    "        Params._dummy(),\n",
    "        \"output_col_cos\",\n",
    "        \"output column name for cos wave.\",\n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "    timestamp_part = Param(\n",
    "        Params._dummy(),\n",
    "        \"timestamp_part\",\n",
    "        \"part of the timestamp like month, day, hour, minute\",\n",
    "        typeConverter=TypeConverters.toString\n",
    "    )\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_col: str = \"input\",\n",
    "        output_col_sin: str = \"sin\",\n",
    "        output_col_cos: str = \"cos\",\n",
    "        timestamp_part: str = \"month\"\n",
    "    ):\n",
    "        super(TimeEncoderTransformer, self).__init__()\n",
    "        self._setDefault(\n",
    "            input_col=None,\n",
    "            output_col_sin=None,\n",
    "            output_col_cos=None,\n",
    "            timestamp_part=None\n",
    "        )\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def set_params(\n",
    "        self,\n",
    "        input_col: str = \"input\",\n",
    "        output_col_sin: str = \"sin\",\n",
    "        output_col_cos: str = \"cos\",\n",
    "        timestamp_part: str = \"month\"\n",
    "    ):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    def get_input_col(self):\n",
    "        return self.getOrDefault(self.input_col)\n",
    "\n",
    "    def get_output_col_sin(self):\n",
    "        return self.getOrDefault(self.output_col_sin)\n",
    "\n",
    "    def get_output_col_cos(self):\n",
    "        return self.getOrDefault(self.output_col_cos)\n",
    "\n",
    "    def get_timestamp_part(self):\n",
    "        return self.getOrDefault(self.timestamp_part)\n",
    "\n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_col = self.get_input_col()\n",
    "        output_col_sin = self.get_output_col_sin()\n",
    "        output_col_cos = self.get_output_col_cos()\n",
    "        timestamp_part = self.get_timestamp_part()\n",
    "\n",
    "        if timestamp_part == 'month':\n",
    "            denominator = 12\n",
    "        elif timestamp_part == 'day':\n",
    "            denominator = 31\n",
    "        elif timestamp_part in ['hour', 'minute']:\n",
    "            denominator = 60\n",
    "        else:\n",
    "            raise Exception()\n",
    "\n",
    "        sin_udf = F.udf(\n",
    "            lambda x: math.sin(2 * math.pi * x / denominator),\n",
    "            DoubleType()\n",
    "        )\n",
    "        cos_udf = F.udf(\n",
    "            lambda x: math.cos(2 * math.pi * x / denominator),\n",
    "            DoubleType()\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(output_col_sin, sin_udf(F.col(input_col)))\n",
    "        df = df.withColumn(output_col_cos, cos_udf(F.col(input_col)))\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab58de8b-bcaa-4544-be4d-646ef5f3de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+---------------+--------------------+--------------------+--------------------+----+---+----+------+-------------------+------------------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count|       category|            desc_enc|             var_enc|            name_enc|year|day|hour|minute|          month_sin|         month_cos|\n",
      "+---------+--------+-----+-----+---------+------------+----------+---------------+--------------------+--------------------+--------------------+----+---+----+------+-------------------+------------------+\n",
      "|839669074|16174997| 2.52| 2000|     true|       false|         0|Pet Accessories|[-0.3230378506332...|[-0.0945925083942...|[0.33843006609151...|2018| 16|  13|    32|0.49999999999999994|0.8660254037844387|\n",
      "|839669045|16174997|  2.5| 2000|     true|       false|         0|Pet Accessories|[-0.3198394560725...|[-0.2202361963689...|[0.33843006609151...|2018| 16|  13|    32|0.49999999999999994|0.8660254037844387|\n",
      "+---------+--------+-----+-----+---------+------------+----------+---------------+--------------------+--------------------+--------------------+----+---+----+------+-------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_col = \"month\"\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    TimeEncoderTransformer(\n",
    "        input_col=input_col,\n",
    "        output_col_sin=f\"{input_col}_sin\",\n",
    "        output_col_cos=f\"{input_col}_cos\",\n",
    "        timestamp_part=input_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de79a222-07c1-4a13-94d5-79f200098751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+----------------+--------------------+--------------------+--------------------+----+----+------+---------+--------------------+------------------+-------------------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count|        category|            desc_enc|             var_enc|            name_enc|year|hour|minute|month_sin|           month_cos|           day_sin|            day_cos|\n",
      "+---------+--------+-----+-----+---------+------------+----------+----------------+--------------------+--------------------+--------------------+----+----+------+---------+--------------------+------------------+-------------------+\n",
      "|185120551|16503999|  8.0|    5|    false|       false|         0|Mobile & Gadgets|[0.41284126639366...|[-0.4695885355273...|[0.54840807616710...|2017|  19|    26|      1.0|6.123233995736766...|-0.651372482722222|-0.7587581226927911|\n",
      "|169964291|16503999|  8.0|    4|    false|       false|         0|Mobile & Gadgets|[0.52240291711958...|[-0.4794806037098...|[0.54840807616710...|2017|   2|    49|      1.0|6.123233995736766...|0.9884683243281114| 0.1514277775045767|\n",
      "+---------+--------+-----+-----+---------+------------+----------+----------------+--------------------+--------------------+--------------------+----+----+------+---------+--------------------+------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_col = \"day\"\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    TimeEncoderTransformer(\n",
    "        input_col=input_col,\n",
    "        output_col_sin=f\"{input_col}_sin\",\n",
    "        output_col_cos=f\"{input_col}_cos\",\n",
    "        timestamp_part=input_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e192bd2a-ebef-4e6f-90a3-b224d131a45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+-----+---------+------------+----------+-----------------+--------------------+--------------------+--------------------+----+------+---------+--------------------+------------------+------------------+------------------+------------------+\n",
      "|    itemid|  shopid|price|stock|cb_option|is_preferred|sold_count|         category|            desc_enc|             var_enc|            name_enc|year|minute|month_sin|           month_cos|           day_sin|           day_cos|          hour_sin|          hour_cos|\n",
      "+----------+--------+-----+-----+---------+------------+----------+-----------------+--------------------+--------------------+--------------------+----+------+---------+--------------------+------------------+------------------+------------------+------------------+\n",
      "|1015202788|16174997| 2.24| 1000|     true|       false|         0|Sports & Outdoors|[-0.3909013677956...|[0.13756516203284...|[0.37490432438525...|2018|    58|      1.0|6.123233995736766...|-0.848644257494751|0.5289640103269624|0.6691306063588582|0.7431448254773942|\n",
      "|1015202786|16174997| 2.58| 1000|     true|       false|         0|Sports & Outdoors|[-0.4445349556409...|[0.01176970079541...|[0.37490432438525...|2018|    58|      1.0|6.123233995736766...|-0.848644257494751|0.5289640103269624|0.6691306063588582|0.7431448254773942|\n",
      "+----------+--------+-----+-----+---------+------------+----------+-----------------+--------------------+--------------------+--------------------+----+------+---------+--------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_col = \"hour\"\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    TimeEncoderTransformer(\n",
    "        input_col=input_col,\n",
    "        output_col_sin=f\"{input_col}_sin\",\n",
    "        output_col_cos=f\"{input_col}_cos\",\n",
    "        timestamp_part=input_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62ad55db-7751-45c1-a1da-53e6694605db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+---------------+--------------------+--------------------+--------------------+----+-------------------+------------------+--------------------+-------------------+------------------+-------------------+--------------------+-------------------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count|       category|            desc_enc|             var_enc|            name_enc|year|          month_sin|         month_cos|             day_sin|            day_cos|          hour_sin|           hour_cos|          minute_sin|         minute_cos|\n",
      "+---------+--------+-----+-----+---------+------------+----------+---------------+--------------------+--------------------+--------------------+----+-------------------+------------------+--------------------+-------------------+------------------+-------------------+--------------------+-------------------+\n",
      "|839669074|16174997| 2.52| 2000|     true|       false|         0|Pet Accessories|[-0.3230378506332...|[-0.0945925083942...|[0.33843006609151...|2018|0.49999999999999994|0.8660254037844387|-0.10116832198743204|-0.9948693233918952|0.9781476007338057|0.20791169081775923|-0.20791169081775907|-0.9781476007338057|\n",
      "|839669045|16174997|  2.5| 2000|     true|       false|         0|Pet Accessories|[-0.3198394560725...|[-0.2202361963689...|[0.33843006609151...|2018|0.49999999999999994|0.8660254037844387|-0.10116832198743204|-0.9948693233918952|0.9781476007338057|0.20791169081775923|-0.20791169081775907|-0.9781476007338057|\n",
      "+---------+--------+-----+-----+---------+------------+----------+---------------+--------------------+--------------------+--------------------+----+-------------------+------------------+--------------------+-------------------+------------------+-------------------+--------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_col = \"minute\"\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    TimeEncoderTransformer(\n",
    "        input_col=input_col,\n",
    "        output_col_sin=f\"{input_col}_sin\",\n",
    "        output_col_cos=f\"{input_col}_cos\",\n",
    "        timestamp_part=input_col\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c6704-dd75-4784-b6dd-b0532d339d85",
   "metadata": {},
   "source": [
    "### Encode Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f12e342a-3beb-410f-8e23-cdba9d9f2b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+-----+---------+------------+----------+--------------------+--------------------+--------------------+----+------------------+--------------------+------------------+-------------------+------------------+-------------------+-------------------+-------------------+----------------+\n",
      "|   itemid|  shopid|price|stock|cb_option|is_preferred|sold_count|            desc_enc|             var_enc|            name_enc|year|         month_sin|           month_cos|           day_sin|            day_cos|          hour_sin|           hour_cos|         minute_sin|         minute_cos|category_indexed|\n",
      "+---------+--------+-----+-----+---------+------------+----------+--------------------+--------------------+--------------------+----+------------------+--------------------+------------------+-------------------+------------------+-------------------+-------------------+-------------------+----------------+\n",
      "|214336190|16581681| 95.0|    5|    false|       false|         0|[-0.1055506967446...|[-0.0046485066413...|[0.01483171526342...|2017|0.8660254037844387| -0.4999999999999998|0.8978045395707416|-0.4403941515576344|0.8660254037844387|-0.4999999999999998| 0.8090169943749475| -0.587785252292473|            20.0|\n",
      "| 69751570|10136611|  8.0|  222|     true|       false|         0|[0.57076718521285...|[-0.0046485066413...|[0.20528068542480...|2016|              -1.0|-1.83697019872102...| -0.72479278722912| 0.6889669190756865|0.9945218953682733|0.10452846326765346|-0.9135454576426011|0.40673664307579976|            20.0|\n",
      "+---------+--------+-----+-----+---------+------------+----------+--------------------+--------------------+--------------------+----+------------------+--------------------+------------------+-------------------+------------------+-------------------+-------------------+-------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "input_col = \"category\"\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    StringIndexer(\n",
    "        inputCol=input_col,\n",
    "        outputCol=f\"{input_col}_indexed\")\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items)\\\n",
    "            .transform(items)\\\n",
    "            .drop(input_col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eadcc6-c656-440b-9411-386b6e3fe595",
   "metadata": {},
   "source": [
    "### Assemble Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f43c335-b332-4a9f-b94e-212ca42ad705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|category_indexed|            features|\n",
      "+----------------+--------------------+\n",
      "|            10.0|[7.44783814E8,1.6...|\n",
      "|            10.0|[7.44783812E8,1.6...|\n",
      "+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "inputCols = [\n",
    "    'itemid',\n",
    "    'shopid',\n",
    "    'price',\n",
    "    'stock',\n",
    "    'cb_option',\n",
    "    'is_preferred',\n",
    "    'sold_count',\n",
    "    'year',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'hour_sin',\n",
    "    'hour_cos',\n",
    "    'minute_sin',\n",
    "    'minute_cos',\n",
    "    'name_enc',\n",
    "    'desc_enc',\n",
    "    'var_enc'\n",
    "]\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    VectorAssembler(\n",
    "        inputCols=inputCols,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "])\n",
    "\n",
    "items = pipeline.fit(items).transform(items)\n",
    "\n",
    "for col in inputCols:\n",
    "    items = items.drop(col)\n",
    "\n",
    "items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7791c-ef0e-4386-8346-afacbe9f59bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Split the input dataset into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12c5d13b-f541-48be-b107-0aba4445c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = items.randomSplit([0.8, 0.2], seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "679f431d-eb93-464d-9756-1af24ad9a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.select(\"features\", \"category_indexed\")\\\n",
    "    .repartition(4)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "!hdfs dfs -get project/data/train/*.json ../data/train\n",
    "\n",
    "test_data.select(\"features\", \"category_indexed\")\\\n",
    "    .repartition(4)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/test\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "!hdfs dfs -get project/data/test/*.json ../data/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10362844-9fb9-41ca-bc2d-3b5fc8c69897",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Select two types of ML models based on the ML task specified in project.info sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab7668-d720-4fd6-a6b4-ee4e31e96cee",
   "metadata": {},
   "source": [
    "1. Logistic Regression\n",
    "2. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9de1a2-7e15-4a08-962b-57e84d062591",
   "metadata": {
    "tags": []
   },
   "source": [
    "## First model type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23c396-50ee-4662-a897-63aaa616250a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9ac0681-8cea-42ae-bcea-7ee86f029368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"category_indexed\", featuresCol=\"features\")\n",
    "model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5654cdf-a871-40c2-b527-1044ba98d0dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75c55976-683c-4ec8-98ff-cafe0054d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9905960b-b4a6-43dd-86da-1afb7701e8f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "320e2fc5-bd71-49e3-9ce8-7f4a0317c947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1-score: 0.6249912365399729\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate the model using F1-score\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(f\"Test set F1-score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f2c67-53f3-4ec0-864b-db93c1e2b76d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specify at least 2 hyperparameters for it and the settings of grid search and cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f17c6f-7faa-4fbc-aa0c-52e566505e61",
   "metadata": {},
   "source": [
    "1. RegParam\n",
    "2. ElasticNetParam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7c361-11d5-4813-9ead-3b3f639f1ddf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optimize its hyperparameters using cross validation and grid search on the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f83c9260-b4e9-4fc9-b704-5d388117bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c1d33d6-5b53-45cf-b043-d4cc49b435dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cc21e7c-e08b-43dd-8a79-072bfe2b883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Create the CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e70b5fcf-91e5-4269-be90-0d6aa6aa7a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using cross-validation on the training data\n",
    "cvModel = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08330fc-2ce5-43f0-a33b-a52f5db35ff2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select the best model (model1) from grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2c1c2bf-cf84-457d-b836-7136585fdec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55fb0ad-2da3-426e-94bc-e30e4ff3f101",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save the model1 to HDFS in location like project/models/model1 and later put it in models/model1 folder in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22d2d42a-822d-4a35-8cde-49d2a6edda3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.write().overwrite().save(\"project/models/model1\")\n",
    "\n",
    "!hdfs dfs -get project/models/model1 ../models/model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf9e9b-c132-4f09-9ec8-36e3272aa36a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict for the test data using the model1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "797053a9-23c5-4b42-8920-e22747786d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = bestModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db097221-af2f-481e-866f-7b55ca7829fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save the prediction results in HDFS in a CSV file like project/output/model1_predictions and later save it in output/model1_predictions.csv folder in the repository…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d7e7b36-4370-4857-8b7f-9c3dc20ced33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+--------------------+----------+\n",
      "|category_indexed|            features|       rawPrediction|         probability|prediction|\n",
      "+----------------+--------------------+--------------------+--------------------+----------+\n",
      "|            14.0|[219027.0,48649.0...|[-10.391423429088...|[2.71367267055964...|      14.0|\n",
      "|            14.0|[516402.0,103034....|[-6.4626236775148...|[4.78846833777391...|      14.0|\n",
      "|            14.0|[686736.0,227608....|[-2.1462063460509...|[7.48238783668572...|       1.0|\n",
      "|            14.0|[947712.0,227608....|[-2.0913311206873...|[7.45067917522111...|       6.0|\n",
      "|            14.0|[1250848.0,10017....|[-0.6827106799718...|[0.00923698282475...|       4.0|\n",
      "|            14.0|[1498027.0,12755....|[-9.6460551350904...|[3.84640026377883...|       1.0|\n",
      "|            14.0|[2870492.0,138778...|[0.88468505871031...|[0.03350637985424...|       7.0|\n",
      "|            14.0|[3274086.0,15575....|[-0.4706152891258...|[1.97179026148674...|       7.0|\n",
      "|            14.0|[3993952.0,141971...|[-7.0692755707060...|[2.13088700654938...|       1.0|\n",
      "|            14.0|[4200053.0,177378...|[-3.0280946031269...|[7.20163020311848...|       1.0|\n",
      "|            14.0|[5370985.0,227608...|[-1.1025692737936...|[0.00339387244012...|       6.0|\n",
      "|            14.0|[5419765.0,220988...|[-2.9769786320869...|[2.59512667467442...|       6.0|\n",
      "|            14.0|[6349023.0,261879...|[-1.9216272545016...|[0.00140074118155...|      11.0|\n",
      "|            14.0|[6697085.0,183504...|[-0.6601778200057...|[0.00539236539524...|       5.0|\n",
      "|            14.0|[7882327.0,227608...|[-1.4624122240390...|[0.00180336436190...|       1.0|\n",
      "|            14.0|[8213211.0,289774...|[2.18367923462005...|[0.03324570034202...|       4.0|\n",
      "|            14.0|[9685293.0,140378...|[-0.7811809392001...|[0.00253171189553...|       5.0|\n",
      "|            14.0|[9686213.0,140378...|[-0.8142164486074...|[0.00248053042929...|       5.0|\n",
      "|            14.0|[9882453.0,295572...|[0.36538500608378...|[0.01270166737284...|      13.0|\n",
      "|            14.0|[1.1550345E7,3992...|[-0.3852329416232...|[8.13382633053225...|       7.0|\n",
      "+----------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a40653d7-9e69-462c-a2c6-21424ab87015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select only the \"label\" and \"prediction\" columns\n",
    "# predictions_selected = predictions.select(\"category_indexed\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6530cc8a-b901-4542-a25e-3c83b354f34f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o7809.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3992.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3992.0 (TID 136483) (hadoop-04.uni.innopolis.ru executor 5): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$17(FileFormatWriter.scala:239)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 332 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:118)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:314)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1525)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:228)\n\t... 41 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$17(FileFormatWriter.scala:239)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 332 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:118)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:314)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1525)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\t... 9 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-304577ec5f03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"project/output/model1_predictions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# predictions.select(\"category_indexed\", \"prediction\")\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o7809.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3992.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3992.0 (TID 136483) (hadoop-04.uni.innopolis.ru executor 5): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$17(FileFormatWriter.scala:239)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 332 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:118)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:314)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1525)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:228)\n\t... 41 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$17(FileFormatWriter.scala:239)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 332 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:118)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:314)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1525)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\t... 9 more\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"category_indexed\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .save(\"project/output/model1_predictions\")\n",
    "\n",
    "# predictions.select(\"category_indexed\", \"prediction\")\\\n",
    "#             .coalesce(1)\\\n",
    "#             .write\\\n",
    "#             .csv(\n",
    "#                 \"project/output/model1_predictions\",\n",
    "#                 mode=\"overwrite\",\n",
    "#                 header=True\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb1793b5-4c64-4b45-83f2-b4341d6d0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get project/output/model1_predictions/* ../output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a8e45-87bc-4bae-8b21-0fd6cb821186",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the best model (model1) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "460a5e41-3c58-4589-ba99-0a0ef83ddcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1-score: 0.6267883961740834\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = bestModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model using F1-score\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(f\"Test set F1-score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38811325-2aa4-4c2d-9b4e-5f42aaab65bf",
   "metadata": {},
   "source": [
    "## Second model type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3df8a1-006b-4ad3-a486-1c509b6076aa",
   "metadata": {},
   "source": [
    "### Build and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de23c149-d192-4581-94ad-c2bbed3db2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"category_indexed\", featuresCol=\"features\")\n",
    "model = dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b95ff-d240-45c9-b99d-f66dde6f1a50",
   "metadata": {},
   "source": [
    "### Predict for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ae9ce4e-e4be-4ead-b5d9-759b604a0e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f4e2e-c175-48d8-8c6b-b75335fc7558",
   "metadata": {},
   "source": [
    "### Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7d9fd9c-f3bd-4c13-842a-fc590febcde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1-score: 0.47410982613515745\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate the model using F1-score\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(f\"Test set F1-score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d913f70-08ce-4971-bde0-e168bb0c9ef7",
   "metadata": {},
   "source": [
    "### Specify at least 2 hyperparameters for it and the settings of grid search and cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d494b3-0909-4376-8956-bd391b14e5fe",
   "metadata": {},
   "source": [
    "1. MaxDepth\n",
    "2. MaxInfoGain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d1ae4-3698-4a9e-95f9-4ee467720d41",
   "metadata": {},
   "source": [
    "### Optimize its hyperparameters using cross validation and grid search on the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d92a7476-aa02-49b3-947e-3ec089122410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(dt.minInfoGain, [0.0, 0.01, 0.1]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff1a8d51-669a-4836-84d5-50d444b2c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17bb9d36-3cb9-42c1-bb00-fb08fecf7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Create the CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96ca761a-bf01-44d2-957b-cc085e00d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using cross-validation on the training data\n",
    "cvModel = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5bdb12-3bb2-4943-9e43-fa62f968b6bc",
   "metadata": {},
   "source": [
    "### Select the best model (model2) from grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a15404c-4cf3-4728-a905-145b384730c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6a73f-8b4a-44e1-b7f0-4c567f62d695",
   "metadata": {},
   "source": [
    "### Save the model2 to HDFS in location like project/models/model2 and later put it in models/model2 folder in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1db79063-ffc0-492f-8711-162ab64fca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.write().overwrite().save(\"project/models/model2\")\n",
    "\n",
    "!hdfs dfs -get project/models/model2 ../models/model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8359d-8b4b-4cfb-919a-76a16b7c48cd",
   "metadata": {},
   "source": [
    "### Predict for the test data using the model2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9736139-4e55-4f35-a36c-ccf05091a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = bestModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745dbe0-04cd-4f59-95f6-74c132c70117",
   "metadata": {},
   "source": [
    "### Save the prediction results in HDFS in a CSV file like project/output/model2_predictions and later save it in output/model2_predictions.csv folder in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8eb428d7-dd67-4c85-a542-f9bdf55c4e59",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o14180.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9915.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9915.0 (TID 339556) (hadoop-04.uni.innopolis.ru executor 8): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$17(FileFormatWriter.scala:239)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 332 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:118)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:314)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1525)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:228)\n\t... 41 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$17(FileFormatWriter.scala:239)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 332 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:118)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:314)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1525)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\t... 9 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-86f5e9a28832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"project/output/model2_predictions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hdfs dfs -get project/output/model1_predictions/* ../output/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o14180.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9915.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9915.0 (TID 339556) (hadoop-04.uni.innopolis.ru executor 8): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$17(FileFormatWriter.scala:239)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 332 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:118)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:314)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1525)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\t... 9 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:228)\n\t... 41 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:331)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$17(FileFormatWriter.scala:239)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 332 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:118)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:431)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:450)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:485)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:314)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1525)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n\t... 9 more\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"category_indexed\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .save(\"project/output/model2_predictions\")\n",
    "\n",
    "!hdfs dfs -get project/output/model1_predictions/* ../output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f48cd-c943-4316-a5fc-944386d66f42",
   "metadata": {},
   "source": [
    "### Evaluate the best model (model2) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f49e0175-6f8b-4ee9-9f07-0968353e1891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1-score: 0.6268764024299281\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = bestModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model using F1-score\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"category_indexed\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(f\"Test set F1-score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4791f-8713-47e6-96e5-15a6e7618ec1",
   "metadata": {},
   "source": [
    "## Compare the models (model1, model2) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe1501d1-82df-4a8f-a4aa-46632611f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"model\", StringType(), False),\n",
    "    StructField(\"f1_score\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"Logistic Regression\", 0.6267883961740834),\n",
    "    (\"Decision Tree\", 0.47410982613515745)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35fd7349-8810-4764-ad41-32374ed1339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get: `../output/_SUCCESS': File exists\n"
     ]
    }
   ],
   "source": [
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .save(\"project/output/evaluation\")\n",
    "\n",
    "!hdfs dfs -get project/output/evaluation/* ../output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa3d91-e67e-4c45-adee-e34a07717557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
